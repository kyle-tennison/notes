# 2. Discrete Random Variables


## 2.1 Basics

A **random variable** is a mapping form the sample space to a definite, scalar quantity.

For example:
- The sum of thee dice rolls
- The number of heads after flipping 10 times
- Literally anything that can be resolved to a numeric value.

A random variable is **discrete** when there are a finite number of values it can take on. The two examples above are finite; an example of a non-discrete random variable is the experiment where a value is chosen $[0,1]$; there are an infinite number of outcomes here.

## 2.2 Probability Mass Functions

PMFs, for short, give the probability of each value that the discrete random variable can take.

### Notation

- The PMF of a random variable $X$ is $p_X$. 
- Capital symbols generally denote random variables (e.g. $X$), while lowercase symbols generally denote the *individual quantities* that the random variable may attain (e.g. $x$)
- In general, 
	$$p_X(x) = P(\{X=x\})$$
- Often, the following notation is used in abbreviation:
	$$P(X=x) \equiv P(\{X=x\})$$
- When evaluating the probability of $X$ evaluating to one of multiple random variables in a set $S = \{x_1,x_2,\dots\}$ we say:
	$$P(X \in S) = \sum_{x\in S}p_X(x)$$
	i.e., you just sum the probability of each. 


### Other Notes

To satisfy [Kolmogorov's probability axioms](1.%20Sample%20Space%20&%20Probability.md#Kolmogorov's%20probability%20axioms), we know that:

$$\sum _{x}p_X(x)=1$$
$$p_X(x)\ge 0$$ 
And, from above:

$$P(X \in S) = \sum_{x\in S}p_X(x)$$
^pmf-sum

> We can do this because all of the $x$'s are mutually exclusive (X evaluates to one thing at a time). 


### Bernoulli Random Variables

A Bernoulli random variable will have two possible outcomes:

$$X = \begin{cases}0\\1\end{cases}$$

And, by the [Compliment Rule](1.%20Sample%20Space%20&%20Probability.md#^compliment-rule), you can say:

$$p_X(k) = \begin{cases}p, &\text{if } k=1,\\1-p, &\text{if }k=0.\end{cases}$$

For example, let $X=0$ be heads and $X=1$ be tails. The probability of a tails is $p=0.5$, so, the probability of a heads is therefore $1-p=1-0.5=0.5$, which is obviously true.

### Binomial Random Variable

If we have a [Bernoulli Random Variable](#Bernoulli%20Random%20Variables), and we run multiple trials, we will end up with more complex scenarios. 

If we are interested with finding the *probability of k successes* (i.e. where $X=1$) *after n trials*, we may use a Binomial Random Variable. This is defined as:

$$p_X(k) = P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$$

For example, if we want to find the probability of *exactly* 5 heads after 8 coin flips, we would evaluate:

$$p_X(5) = \binom{8}{5}(0.5)^5(0.5)^{3}=\frac{7}{32}$$

We can see, pretty intuitively, that the probability of getting 5 *or more* heads would be:

$$p_X(5)+p_X(6)+p_X(7)+p_X(8)$$


We can do this because of the [equation above](#^pmf-sum). More generally, this can be expressed as:

If we are interested in finding the *probability of k **or more** successes* (i.e. where $X=1$) *after n trials*, we may use the following:
$$P(X\ge k) = \sum_{i=k}^n \binom{n}{i}p^i(1-p)^{n-i}$$

### Geometric Random Variable


The geometric random variable gives the probability of a success occurring *for the first time* after $X$ trials. This is given by:

$$p_X(k) =(1-p)^{k-1}p$$

For example, the probability of getting a heads *for the first time* after one coin flip is:

$$p_X(1) = \cancelto{1}{(1-p)^{1-1}} (0.5)=(0.5)$$

Which agrees with our intuition. Now, we would expect the probability of getting the first heads after *five* coin flips to be much lower, because it's quite likely that one of the first four head flips will contain a heads. We can evaluate this to make sure:

$$p_X(5) = (1-0.5)^{5-1}(0.5)=0.03125$$

Which again agrees with our intuition. 

### Poisson Random Variable

The Poisson Random Variable is an approximation for the [Binomial Random Variable](#Binomial%20Random%20Variable) where $n$ is very large and $p$ is very small. 


$$p_X(k) = e^{-\lambda } \frac{\lambda ^k}{k!}$$

where $\lambda = np$.

For example, if $p=0.01$, $n=100$, and $k=5$, the binomial PMF is:

$$p_X(5)=\binom{100}{5}(0.01)^5(1-0.01)^{100-5}\approx0.00290$$

And the Poisson PMF, with $\lambda = 1$, is:

$$p_X(5) = e^{-1}\frac{1^5}{5!} = \frac{1}{120e} \approx 0.00307$$

## 2.3 Functions of Random Variables

If $X$ is a random variable, a function $g(X)=Y$ may be defined. Some things to note:

- If $X$ is discrete, $Y$ is also discrete
- $g(\cdot)$ may be any functionâ€”linear or nonlinear
- $$p_Y(y) = \sum_{\{x|g(x)=y\}}p_X(x)$$
	- In the cases where two $X$ values correspond to a single y value, you sum the probabilities. Otherwise, you just map them 1:1.
^single-var-function

We can think of $Y$ as just another mapping from $\Omega \to \mathbb R$. 


## 2.4 Expectation, Mean, and Variance

The **expectation/mean** is simply:

$$\mathbf{E}[X] = \sum_xx p_X(x)$$

This evaluates to a *scalar*. 

This is derived with the following:

--- 

Say $x_i$ is an outcome you achieve $k_i$ times. The weighted average outcome is:

$$\frac{x_0k_0+x_1k_1+\cdots +x_nk_n}{k}$$

where $k$ is the total number of outcomes. As $n \to \infty$, we'll notice that $k_i/k \to p_X(x_i)$. This should hopefully be intuitive; if we flip heads over and over, the ratio of heads vs the number of tosses will eventually approach 0.5. If you substitute this in, you get:

$$x_0p_X(x_0) + x_1p_X(x_1)+\cdots+x_np_X(x_n) = \sum_xxp_X(x)$$

So intuitively:
> If you repeat the experiment an infinite number of times, you would eventually converge upon the expectation/mean. 

--- 

You can think of the mean as the center of gravity of the PMF. If the PMF is symmetric, it must be symmetric about the mean. 

### Variance, Moments, and the Expected Value Rule

There are different **moments**. The *first moment* is the mean/expectation. Higher order moments are defined as:

$$\text{``}n^{th} \text{ order moment"} = \mathbf E[X^n]$$

In english, the nth order moment is the expectation/mean of the power function of the random variable.

The **variance** is defined as:

$$\text{var}(X) \equiv \mathbf E \Big[ \big(X- \mathbf E[X]\big)^2 \Big].$$

The expression $X- \mathbf E[X]$  evaluates to another *random variable*, which we then take the second moment of. Because the second moment has a even power, we can assert that the variance will always be nonnegative.

The **standard deviation** is closely linked to the variance:

$$\sigma _X \equiv \sqrt{\text{var}(X)}.$$

### Expected Value Rule

The expected value rule gives an easy way to compute the expected value of a *function* of a random variable.

$$\mathbf E[g(X)] = \sum_x g(x)p_X(x)$$

The following is a derivation:

--- 

$$
\begin{aligned}
\mathbf E[g(X)] &= \mathbf E[Y]\\
&=\sum_y yp_Y(y)\\
&= \sum_y y \underbrace{\sum_{\{x|g(x)=y\}}p_X(x)}_{\text{definition of }p_Y(y)} \\\\
&= \sum_y \sum_{\{x|g(x)=y\}} y\,p_X(x) \qquad \text{(move sides)}\\\\
&= \sum_y \sum_{\{x|g(x)=y\}}\,\, \underbrace{g(x)}_{y=g(x)}p_X(x)\\\\
& \text{Which is the same as saying:}\\\\
&= \sum_xg(x)p_X(x)
\end{aligned}
$$
---

This gives us an abbreviation of the variance equation:

$$\text{var}(X) = \sum_x (x-\mathbf E[X])^2p_X(x).$$

which can equivalently be expressed (p87) as:

$$\text{var}(X) = \mathbf E[X^2] - (\mathbf E[X])^2$$

Similarly, the $n^{th}$ moment can be found with:

$$\mathbf E[x^n] = \sum_x x^n p_X(x).$$


### Properties of Mean and Variance

If a random variable $Y$ is a linear function of a random variable $X$, where:

$$Y = aX + b$$

Then, by using the [Expected Value Rule](#Expected%20Value%20Rule) we can say:

$$\mathbf E [Y] = a\mathbf E[X] +b$$

and

$$\text{var}(Y)= a^2 \text{var}(X)$$

## 2.5 Joint PMFs of Multiple Random Variables

A PMF may contain two random variables.

$$p_{X,Y}(x,y) \equiv \underbrace{\mathbf P(\{X=x\} \cap \{ Y=y\})}_{\text{often shown as: } \mathbf P(X=x, Y=y)}$$


If $A$ is a set of all $(x,y)$ pairs that exhibit an attribute of interest, then we can say:

$$\mathbf P((X,Y)\in A) = \sum_{(x,y) \in A}p_{X,Y}(x,y)$$

Similarly, we can say:

$$p_X(x) = \sum_y p_{X,Y}(x,y), \qquad p_Y(y) = \sum_x p_{X,Y}(x,y). $$

This should be intuitive: if you imagine a 2D grid, you may find the PMF in one axis by summing over the other axis. 

### Functions of Multiple Random Variables

If a random variable $Z$ is defined as:

$$Z = g(X, Y)$$

then its PMF would be:

$$p_Z(z) = \sum_{\{(x,y) | g(x,y) = z\}}p_{X,Y}(x,y).$$

for the same reason as explained [for single variable functions](#^single-var-function). 

Some more properties:
- $$\mathbf E [g(X,Y)] = \sum_x \sum_y g(x,y)p_{X,Y}(x,y).$$
- $$\mathbf E[aX + bY + c] = a \mathbf E[X] + b \mathbf E[Y] + c$$

You can create even larger joint PMFs:

- $$p_{X,Y,Z}(x,y,z) \equiv \mathbf P(X=x,Y=y,Z=z)$$
- $$p_X(x) = \sum_y \sum_z p_{X,Y,Z} (x,y,z)$$
- $$\mathbf E [g(X,Y,Z)] = \sum_x\sum_y\sum_z g(x,y,z)p_{X,Y,Z}(x,y,z).$$
- $$\mathbf E[aX + bY +cZ + d] = a \mathbf E[X] + b \mathbf E[Y] + c\mathbf E[Z] + d$$
You can extrapolate these for $n$ dimensions. 



## 2.6 Conditioning

This is very similar to the previously discussed [Conditional Probability](1.%20Sample%20Space%20&%20Probability.md#Conditional%20Probability) section.

A **conditional PMF** is simply:

$$p_{X|A}(x) = \mathbf P(X=x|A) = \frac{\mathbf P(\{X=x\} \cap A )}{\mathbf P(A)}$$
^conditional-pmf

For example:

Let $X$ be the roll of a fair die and $A$ be the event that the roll is an even number.

$$p_{X|A}(k) = \frac{\mathbf P(\{X=k\} \text{ and k is even})}{\mathbf P(A)}$$

We can easily see that $\mathbf P(A) = 0.5$. The probability of $\mathbf P(\{X =k\}) = 1/6;\forall k$, but depending on if the value is even or odd, the value of $\mathbf P(\{X=k\} \text{ and k is even})$ changes:

$$\mathbf P(\{X=k\} \text{ and k is even}) = \cases{
\frac{1}{6}, & \text{if k is even,}\\
0, & \text{otherwise.}
}$$

Then, dividing this by $\mathbf P(A)$ i.e. 0.5 gives:

$$p_{X|A} =\cases{
\frac{1}{3}, & \text{if k is even,}\\
0, & \text{otherwise.}
}$$

### Conditioning one variable on another

We are interested in the case where:

$$p_{X|Y}(x|y) = \mathbf P(X=x \,|\,Y=y)$$

This evaluates to:

$$p_{X|Y}(x|y) = \frac{\mathbf P (X=x, Y=y)}{\mathbf P(Y=y)} = \frac{p_{X,Y}(x,y)}{p_{Y}(y)}$$


You can visualize this pretty easily; you're fixing the y-axis in a 2D pmf to a single given y-value, which reduces it to a 1D pmf. Because it's this simple, it's often more convenient to calculate and you can inversely say:

$$p_{X,Y} (x,y) = p_Y(y)p_{X|Y}(x|y)$$
or
$$p_{X,Y} (x,y) = p_X(y)p_{Y|X}(y|x)$$


This means u can do this to solve for $p_X(x)$:

$$p_X(x) = \sum_y p_{X,Y}(x,y) = \sum_ y p_Y(y)p_{X|Y}(x|y)$$

and the same can obviously be done for $p_Y(y)$. 



### Conditional Expectation

Because a conditional PMF is literally just a 1D pmf that's "derived" from a 2D joint PMF, all of the expectation rules apply the same way. 

$$\mathbf E[X|A] = \sum_x x p_{X|A}(x)$$

$$\mathbf E[g(X)|A] = \sum_x g(x)p_{X|A}(x)$$

$$\mathbf E[X|Y=y] = \sum_x x p_{X|Y}(x|y)$$

These are self explanatory. The following are more specific to this scenario:


- If $A_1, \dots, A_n$ are *disjoint* events that have $\mathbf P(A_i) > 0$ for all $i$, then:
$$\mathbf E[X] = \sum_{i=1}^n \mathbf P(A_i)\mathbf E[X|A_i]$$

	And, furthermore, if another event $B$ is introduced where $\mathbf (A_i \cap B) >0$ for all $i$, this above equation is modified by:
$$\mathbf E[X|B] = \sum_{i=1}^n \mathbf P(A_i|B) \mathbf E[X|A_i \cap B]$$

- Finally, theres:

$$\mathbf E[X] = \sum_y p_Y(y) \mathbf E[X|Y=y]$$

These last 3 equations are all derived from the [total probability theorem](1.%20Sample%20Space%20&%20Probability.md#^total-prob-theorem). 

## 2.7 Independence

In [1.3 Independence](1.%20Sample%20Space%20&%20Probability.md#1.3%20Independence), independence was previously described. The gist of this was:

*If knowing the probability of a certain event influences the probability of another event occurring, then the two are not independent.*

Formally, this was described as:

$$\mathbf P(A \cap B ) = \mathbf P(A)\mathbf  P(B), \quad \text{if independent.}$$

Now, for random variables, this is:

$$\mathbf P(\{X=x \}\cap A) = \mathbf P(X=x) \mathbf P(A) = p_X(x)\mathbf P(A), \quad \forall x$$

If we apply the [conditional PMF](#^conditional-pmf) equation, we'll see:

$$p_{X|A}(X) = \frac{\mathbf P(\{X=x\} \cap A)}{\mathbf P(A)}$$

moving things around gives,

$$\mathbf P(\{X=x\} \cap A )= p_{X|A}(X)\mathbf P(A)  $$

Which shows that

$$p_X(x) = p_{X|A}(x), \quad \text{when independent}$$

which is exactly what we should expect: knowing that $A$ occurred makes no difference.

### Independence of random variables

Two random variables are independent if:

$$p_{X,Y}(x,y) = p_X(x)p_Y(y)$$
for all $x$ and all $y$

They can be **conditionally independent**, such that:

$$p_{X,Y|A}(x,y) = p_{X|A}(x)p_{Y|A}(y)$$

The **expectation** of two independent variables is:

$$\mathbf E[XY] = \mathbf E[X]\mathbf E[Y]$$

and 

$$\mathbf E[g(X)h(Y)] = \mathbf E[g(X)] \;\mathbf E[h(Y)]$$


> derivation provided on p. 110

The **variance** is:

$$\text{var}(X+Y) = \text{var}(X) + \text{var}(Y)$$

> derivation provided on p. 111

### Independence of more than two Random Variables

Multiple random variables are independent if

$$p_{X,Y,Z}(x,y,z) = p_X(x)p_Y(y)p_Z(z), \quad \text{for all }x, y,z.$$

Putting these variables in functions like $f(X)$ and $g(Y)$ does not change independence.

$$\text{var}(X_1 + X_2 + \cdots +X_n) = \text{var}(X_1) + \dots + \text{var}(X_n)$$





## 2.8 Summary /  Reference

### Special Random Variables


**Discrete Uniform over $[a,b]$:**

$$
p_X(k) = \begin{cases}
\frac{1}{b - a + 1}, & \text{if } k = a, a+1, \ldots, b, \\
0, & \text{otherwise},
\end{cases}
$$

$$
\mathbf{E}[X] = \frac{a+b}{2}, \quad \text{var}(X) = \frac{(b-a)(b-a+2)}{12}.
$$

**Bernoulli with Parameter $p$:**
(Describes the success or failure in a single trial.)

$$
p_X(k) = \begin{cases}
p, & \text{if } k = 1, \\
1-p, & \text{if } k = 0,
\end{cases}
$$

$$
\mathbf{E}[X] = p, \quad \text{var}(X) = p(1-p).
$$

**Binomial with Parameters $p$ and $n$:**
(Describes the number of successes in $n$ independent Bernoulli trials.)

$$
p_X(k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k=0,1,\ldots,n,
$$

$$
\mathbf{E}[X] = np, \quad \text{var}(X) = np(1-p).
$$

**Geometric with Parameter $p$:**
(Describes the number of trials until the first success, in a sequence of independent Bernoulli trials.)

$$
p_X(k) = (1-p)^{k-1} p, \quad k = 1, 2, \ldots.
$$

$$
\mathbf{E}[X] = \frac{1}{p}, \quad \text{var}(X) = \frac{1-p}{p^2}.
$$

**Poisson with Parameter $\lambda$:**
(Approximates the binomial PMF when $n$ is large, $p$ is small, and $\lambda = np$.)

$$
p_X(k) = e^{-\lambda} \frac{\lambda^k}{k!}, \quad k=0,1,\ldots.
$$

$$
\mathbf{E}[X] = \lambda, \quad \text{var}(X) = \lambda.
$$


