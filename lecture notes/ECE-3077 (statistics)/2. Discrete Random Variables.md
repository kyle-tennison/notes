# 2. Discrete Random Variables


## 2.1 Basics

A **random variable** is a mapping form the sample space to a definite, scalar quantity.

For example:
- The sum of thee dice rolls
- The number of heads after flipping 10 times
- Literally anything that can be resolved to a numeric value.

A random variable is **discrete** when there are a finite number of values it can take on. The two examples above are finite; an example of a non-discrete random variable is the experiment where a value is chosen $[0,1]$; there are an infinite number of outcomes here.

## 2.2 Probability Mass Functions

PMFs, for short, give the probability of each value that the discrete random variable can take.

### Notation

- The PMF of a random variable $X$ is $p_X$. 
- Capital symbols generally denote random variables (e.g. $X$), while lowercase symbols generally denote the *individual quantities* that the random variable may attain (e.g. $x$)
- In general, 
	$$p_X(x) = P(\{X=x\})$$
- Often, the following notation is used in abbreviation:
	$$P(X=x) \equiv P(\{X=x\})$$
- When evaluating the probability of $X$ evaluating to one of multiple random variables in a set $S = \{x_1,x_2,\dots\}$ we say:
	$$P(X \in S) = \sum_{x\in S}p_X(x)$$
	i.e., you just sum the probability of each. 


### Other Notes

To satisfy [Kolmogorov's probability axioms](1.%20Sample%20Space%20&%20Probability.md#Kolmogorov's%20probability%20axioms), we know that:

$$\sum _{x}p_X(x)=1$$
$$p_X(x)\ge 0$$ 
And, from above:

$$P(X \in S) = \sum_{x\in S}p_X(x)$$
^pmf-sum

> We can do this because all of the $x$'s are mutually exclusive (X evaluates to one thing at a time). 


### Bernoulli Random Variables

A Bernoulli random variable will have two possible outcomes:

$$X = \begin{cases}0\\1\end{cases}$$

And, by the [Compliment Rule](1.%20Sample%20Space%20&%20Probability.md#^compliment-rule), you can say:

$$p_X(k) = \begin{cases}p, &\text{if } k=1,\\1-p, &\text{if }k=0.\end{cases}$$

For example, let $X=0$ be heads and $X=1$ be tails. The probability of a tails is $p=0.5$, so, the probability of a heads is therefore $1-p=1-0.5=0.5$, which is obviously true.

### Binomial Random Variable

If we have a [Bernoulli Random Variable](#Bernoulli%20Random%20Variables), and we run multiple trials, we will end up with more complex scenarios. 

If we are interested with finding the *probability of k successes* (i.e. where $X=1$) *after n trials*, we may use a Binomial Random Variable. This is defined as:

$$p_X(k) = P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$$

For example, if we want to find the probability of *exactly* 5 heads after 8 coin flips, we would evaluate:

$$p_X(5) = \binom{8}{5}(0.5)^5(0.5)^{3}=\frac{7}{32}$$

We can see, pretty intuitively, that the probability of getting 5 *or more* heads would be:

$$p_X(5)+p_X(6)+p_X(7)+p_X(8)$$


We can do this because of the [equation above](#^pmf-sum). More generally, this can be expressed as:

If we are interested in finding the *probability of k **or more** successes* (i.e. where $X=1$) *after n trials*, we may use the following:
$$P(X\ge k) = \sum_{i=k}^n \binom{n}{i}p^i(1-p)^{n-i}$$

### Geometric Random Variable


The geometric random variable gives the probability of a success occurring *for the first time* after $X$ trials. This is given by:

$$p_X(k) =(1-p)^{k-1}p$$

For example, the probability of getting a heads *for the first time* after one coin flip is:

$$p_X(1) = \cancelto{1}{(1-p)^{1-1}} (0.5)=(0.5)$$

Which agrees with our intuition. Now, we would expect the probability of getting the first heads after *five* coin flips to be much lower, because it's quite likely that one of the first four head flips will contain a heads. We can evaluate this to make sure:

$$p_X(5) = (1-0.5)^{5-1}(0.5)=0.03125$$

Which again agrees with our intuition. 

### Poisson Random Variable

The Poisson Random Variable is an approximation for the [Binomial Random Variable](#Binomial%20Random%20Variable) where $n$ is very large and $p$ is very small. 


$$p_X(k) = e^{-\lambda } \frac{\lambda ^k}{k!}$$

where $\lambda = np$.

For example, if $p=0.01$, $n=100$, and $k=5$, the binomial PMF is:

$$p_X(5)=\binom{100}{5}(0.01)^5(1-0.01)^{100-5}\approx0.00290$$

And the Poisson PMF, with $\lambda = 1$, is:

$$p_X(5) = e^{-1}\frac{1^5}{5!} = \frac{1}{120e} \approx 0.00307$$

## 2.3 Functions of Random Variables

If $X$ is a random variable, a function $g(X)=Y$ may be defined. Some things to note:

- If $X$ is discrete, $Y$ is also discrete
- $g(\cdot)$ may be any functionâ€”linear or nonlinear
- $$p_Y(y) = \sum_{\{x|g(x)=y\}}p_X(x)$$
	- In the cases where two $X$ values correspond to a single y value, you sum the probabilities. Otherwise, you just map them 1:1.
^single-var-function

We can think of $Y$ as just another mapping from $\Omega \to \mathbb R$. 


## 2.4 Expectation, Mean, and Variance

The **expectation/mean** is simply:

$$\mathbf{E}[X] = \sum_xx p_X(x)$$

This evaluates to a *scalar*. 

This is derived with the following:

--- 

Say $x_i$ is an outcome you achieve $k_i$ times. The weighted average outcome is:

$$\frac{x_0k_0+x_1k_1+\cdots +x_nk_n}{k}$$

where $k$ is the total number of outcomes. As $n \to \infty$, we'll notice that $k_i/k \to p_X(x_i)$. This should hopefully be intuitive; if we flip heads over and over, the ratio of heads vs the number of tosses will eventually approach 0.5. If you substitute this in, you get:

$$x_0p_X(x_0) + x_1p_X(x_1)+\cdots+x_np_X(x_n) = \sum_xxp_X(x)$$

--- 

You can think of the mean as the center of gravity of the PMF. If the PMF is symmetric, it must be symmetric about the mean. 

### Variance, Moments, and the Expected Value Rule

There are different **moments**. The *first moment* is the mean/expectation. Higher order moments are defined as:

$$\text{``}n^{th} \text{ order moment"} = \mathbf E[X^n]$$

In english, the nth order moment is the expectation/mean of the power function of the random variable.

The **variance** is defined as:

$$\text{var}(X) \equiv \mathbf E \Big[ \big(X- \mathbf E[X]\big)^2 \Big].$$

The expression $X- \mathbf E[X]$  evaluates to another *random variable*, which we then take the second moment of. Because the second moment has a even power, we can assert that the variance will always be nonnegative.

The **standard deviation** is closely linked to the variance:

$$\sigma _X \equiv \sqrt{\text{var}(X)}.$$

### Expected Value Rule

The expected value rule gives an easy way to compute the expected value of a *function* of a random variable.

$$\mathbf E[g(X)] = \sum_x g(x)p_X(x)$$

The following is a derivation:

--- 

$$
\begin{aligned}
\mathbf E[g(X)] &= \mathbf E[Y]\\
&=\sum_y yp_Y(y)\\
&= \sum_y y \underbrace{\sum_{\{x|g(x)=y\}}p_X(x)}_{\text{definition of }p_Y(y)} \\\\
&= \sum_y \sum_{\{x|g(x)=y\}} y\,p_X(x) \qquad \text{(move sides)}\\\\
&= \sum_y \sum_{\{x|g(x)=y\}}\,\, \underbrace{g(x)}_{y=g(x)}p_X(x)\\\\
& \text{Which is the same as saying:}\\\\
&= \sum_xg(x)p_X(x)
\end{aligned}
$$
---

This gives us an abbreviation of the variance equation:

$$\text{var}(X) = \sum_x (x-\mathbf E[X])^2p_X(x).$$

which can equivalently be expressed (p87) as:

$$\text{var}(X) = \mathbf E[X^2] - (\mathbf E[X])^2$$

Similarly, the $n^{th}$ moment can be found with:

$$\mathbf E[x^n] = \sum_x x^n p_X(x).$$


### Properties of Mean and Variance

If a random variable $Y$ is a linear function of a random variable $X$, where:

$$Y = aX + b$$

Then, by using the [Expected Value Rule](#Expected%20Value%20Rule) we can say:

$$\mathbf E [Y] = a\mathbf E[X] +b$$

and

$$\text{var}(Y)= a^2 \text{var}(X)$$

## 2.5 Joint PMFs of Multiple Random Variables

A PMF may contain two random variables.

$$p_{X,Y}(x,y) \equiv \underbrace{\mathbf P(\{X=x\} \cap \{ Y=y\})}_{\text{often shown as: } \mathbf P(X=x, Y=y)}$$


If $A$ is a set of all $(x,y)$ pairs that exhibit an attribute of interest, then we can say:

$$\mathbf P((X,Y)\in A) = \sum_{(x,y) \in A}p_{X,Y}(x,y)$$

Similarly, we can say:

$$p_X(x) = \sum_y p_{X,Y}(x,y), \qquad p_Y(y) = \sum_x p_{X,Y}(x,y). $$

This should be intuitive: if you imagine a 2D grid, you may find the PMF in one axis by summing over the other axis. 

### Functions of Multiple Random Variables

If a random variable $Z$ is defined as:

$$Z = g(X, Y)$$

then its PMF would be:

$$p_Z(z) = \sum_{\{(x,y) | g(x,y) = z\}}p_{X,Y}(x,y).$$

for the same reason as explained [for single variable functions](#^single-var-function). 

Some more properties:
- $$\mathbf E [g(X,Y)] = \sum_x \sum_y g(x,y)p_{X,Y}(x,y).$$
- $$\mathbf E[aX + bY + c] = a \mathbf E[X] + b \mathbf E[Y] + c$$

You can create even larger joint PMFs:

- $$p_{X,Y,Z}(x,y,z) \equiv \mathbf P(X=x,Y=y,Z=z)$$
- $$p_X(x) = \sum_y \sum_z p_{X,Y,Z} (x,y,z)$$
- $$\mathbf E [g(X,Y,Z)] = \sum_x\sum_y\sum_z g(x,y,z)p_{X,Y,Z}(x,y,z).$$
- $$\mathbf E[aX + bY +cZ + d] = a \mathbf E[X] + b \mathbf E[Y] + c\mathbf E[Z] + d$$
You can extrapolate these for $n$ dimensions. 



