# 3. General Random Variables

## 3.1 Continuous Random Variables and PDFs

A continuous random variable is analogous to a [Discrete Random Variable](2.%20Discrete%20Random%20Variables.md), except that its obviously continuous. By definition:

$$\mathbf P (X\in B) = \int_Bf_X(x)\,dx$$

Because $X$ is still a mapping from $\Omega \to \mathbb R$, $B$ is simply an interval (or set of intervals). If $B=[a,b]$, this integral is simply:

$$\mathbf P(X\in [a,b]) = \int \limits _a^bf_X(x)\,dx$$

$f_X(x)$ here is the **probability distribution function (PDF)**. It represents the probability per unit length in the neighborhood of $x$. 

PDFs must always be greater than zero, but they *may momentarily exceed 1* so long that it's over an infinitesimal $\delta x$. It must still be true that $\int_B f_X(x)\,dx \le 1 \forall B$. 

### Expectation, Variance

The expectation of a PDF is:

$$\boxed{\mathbf E[X] \equiv \int \limits_{-\infty }^ \infty x f_X(x)\,dx}$$

Then, similar to the [Expectation and Variance of Discrete Random Variables](2.%20Discrete%20Random%20Variables.md#2.4%20Expectation,%20Mean,%20and%20Variance), we can likewise say:

$$\mathbf E[g(X)] = \int \limits _{- \infty}^ \infty g(x)f_X(x)\,dx$$

$$\text{if}\quad Y=aX+b, \quad \text{then}\quad \mathbf E[Y] = a \mathbf E[X]+b\quad \text{and}\quad \text{var}(Y)=a^2\text{var}(X)$$
$$\text{var}(X) = \underline{\mathbf E\big[(X-\mathbf E[X])^2\big]} = \int \limits _{- \infty}^ \infty \big(x- \mathbf E[X]\big)^2f_X(x)\,dx$$

If you simplify the underlined term above, you can get an expression that is much easier to work with:
$$\text{var}(X) = \mathbf E[X^2] - (\mathbf E[X])^2$$



> Derivations for all of this on p. 144-146

### Exponential Random Variable

An exponential random variable is a special kind of random variable that is very similar to the [Geometric Random Variable](2.%20Discrete%20Random%20Variables.md#Geometric%20Random%20Variable). Its PDF is:

$$f_X(x)\,dx = \cases{\lambda e^{-\lambda x}, {\quad \text{if } x \ge 0}\\0, \qquad \text{otherwise}}$$


This shows us that the probability of $X$ exceeding some value $a$ will decrease exponentially as $x \to \infty$:

$$\mathbf P(X\ge a) = \int_a^ \infty \lambda e^{- \lambda x}\,dx =  -e^{-\lambda x} \Big|^\infty _a = e^{-\lambda a}$$

The mean and variance are:

$$\mathbf E[X] = \frac 1 \lambda, \qquad \text{var}(X)= \frac{1}{\lambda ^2}$$

> Derivation on p.147-148

## 3.2 Cumulative Distribution Functions (CDFs)

A cumulative distribution function applies to both [Discrete Random Variables](2.%20Discrete%20Random%20Variables.md) and Continuous Random Variables.

$$F_X(x) = \begin{cases}
\displaystyle \sum_{k \le x}p_X(k), & \text{if } X \text{ is discrete},\\

\\
\displaystyle \int _{- \infty} ^ xf_X(t)\,dt, & \text{if } X \text{ is continuous}


\end{cases}$$

This is representing the probability $\mathbf P(X\le x)$ for both discrete and continuous variables, which both have a very analogous interpretation. 


You can start with a CDF and derive a PDF/PMF. For PDFs:

$$f_X(x) = \frac{dF_X(x)}{dx}$$
For PMFs:

$$p_X(k) = F_X(k) - F_X(k-1)$$
for all integers $k$. 




### Normal Random Variables

**Gaussian** random variables are another name for normal random variables. These are of the form:

$$\boxed{
f_X(x)= \frac{1}{\sqrt{2\pi}\cdot \sigma}e^{-(x-\mu)^2/2\sigma  ^2}
}$$

where $\mu$ is the mean and $\sigma$ is the standard deviation, such that:

$$\mathbf E[X] = \mu, \qquad \text{var}(X)=\sigma^2.$$

> Derivations on p. 153

Normality is preserved in linear transformations, so that, if $Y = aX+b$, we can say:

$$\mathbf E[Y] = a \mu +b, \qquad \text{var}(Y) = a^2 \sigma^2$$

#### Standard Normal Random Variable

The standard normal random variable $\Phi (y)$ is a normal random variable with $\mu=0$ and $\sigma = 1$. We consider this because these values are often readily available to lookup. We can use a standard table as follows:

First, *"standardize"* the random variable such that:

$$Y = \frac{X- \mu}{\sigma}$$

This makes $\mathbf E[Y] = 0$ and $\text{var}(Y) = 1$. So, when you run this for $x$, you get a standardized value $y$ that you can lookup:

$$y = \frac{x - \mu}{\sigma}$$

Now, in the table, you can lookup $\Phi(Y \le y)$. 

Some tables will only provide positive values (something like $y \in [0, 3.49]$); if $y$ is negative, you can use the following to still get a valid value:

$$\Phi(y) = 1- \Phi(-y)$$

We can do this because $\Phi$ is symmetric about $y=0$. 

## 3.4 Joint PDFs of Multiple Random Variables

This chapter is nearly identical to [2.5 Joint PMFs of Multiple Random Variables](2.%20Discrete%20Random%20Variables.md#2.5%20Joint%20PMFs%20of%20Multiple%20Random%20Variables). 

$$\mathbf P((X,Y) \in B) = \iint \limits_{(x,y) \in B} f_{X,Y}(x,y)\,dx\,dy$$

So, if $B$ is rectangular in the form:

$$B = \{(x,y) \,|\, a \le X \le b, c \le Y \le d \}$$

then:

$$\mathbf P((X,Y) \in B) = \int \limits _c^d \int\limits_a^b f_{X,Y}(x,y)\,dx\,dy$$

The joint PMF $f_{X,Y}(x,y)$ can be thought of as the probability per unit area in the neighborhood of $(x,y)$. 

You can compute the **marginal PDF**, similar to what was done [for PMFs](2.%20Discrete%20Random%20Variables.md#^marginal-pmf) with:

$$f_X(x) = \int \limits _{- \infty} ^ \infty f_{X,Y}(x,y)\, dy$$

$$f_Y(y) = \int \limits _{- \infty} ^ \infty f_{X,Y}(x,y)\, dx$$

### Joint CDFs

CDFs can also have multiple random variables:

$$F_{X,Y}(x,y) = \textbf P(X \le x, Y \le y) = \int_{- \infty}^x \int_{-\infty}^y f_{X,Y}( x^ \star, y ^ \star)\, d y ^\star \, d x^ \star$$

You can go backwards to find the PDF from the CDF:

$$f_{X,Y}(x,y) = \frac{\partial^2F_{X,Y}(x,y)}{\partial x \partial y}$$

### Expectation

Again, like [Discrete Random Variables](2.%20Discrete%20Random%20Variables.md), we have the following analogous properties in PDFs:

$$\mathbf E[g(X,Y)] = \int\limits_{-\infty}^{\infty} \int \limits_{-\infty} ^ \infty g(x,y) f_{X,Y}(x,y)\,dx\,dy$$

$$\mathbf E[aX + bY+c] = a \mathbf E[X] + b \mathbf E[Y] + c$$

### More than Two Random Variables

$$\mathbf P((X,Y,Z) \in B) = \iiint \limits_{(x,y,z) \in B} f_{X,Y,Z}(x,y,z)\,dx\,dy\,dz$$

You can reduce the dimensions by integrating out any one:
$$f_{X,Y}(x,y) = \int \limits_{-\infty}^{\infty}f_{X,Y,Z}(x,y,z) \,dz$$

Or isolate a single dimension by integrating out any number:

$$f_X(x) = \int_{-\infty} ^\infty \int_{-\infty} ^\infty f_{X,Y,Z}(x,y,z)\,dy\,dz$$

Expectation remains the same:

$$\mathbf E[aX + bY + cZ] = a \mathbf E[X] + b\mathbf E[Y] + c \mathbf E[Z]$$

You can continue this to higher orders beyond $\mathbb R^3$. 

## 3.5 Conditioning

A conditional PDF of a single variable is again just a PDF but in a different "universe" (as the book calls it) from the original problem.

$$\mathbf P(X\in B |A) = \int_Bf_{X|A}(x)\,dx$$

If we consider the event where $f_{X|A} = f_{X|\{X\in S\}}$, meaning that the event $A$ is defined by $X$ belonging to subset $S$. This is notated as:

$$\mathbf P(X\in B \,|\,X \in S)$$

However, this is not always the case. If the event $A$ does not involve the variable $X$, then the two are simply independent:

$$\mathbf P(X\in B |A ) = \mathbf P(X \in B), \quad \text{if } A \text{ does not depend on } X$$

*If* this is the case, then the following equation will not work because $\mathbf P(X \in S)=0$ and the formula goes undefined. However, for all other cases where $A=\{X \in S\}$, the equation holds.

$$\mathbf P(X\in B| X \in S) = \frac{\mathbf P(X\in B, X \in S)}{\mathbf P(X \in S)} = \frac{\displaystyle \int_{S\cap B} f_X(x)\,dx}{\displaystyle \int_Sf_X(x)\,dx }$$

> In the book, $\mathbf P(X\in S)$ is shown as $\mathbf P(X \in A)$; however I find it easy to confuse subset $A$ with event $A$. 

Hence, the conditional PDF is:

$$f_{X|\{X \in S\}} (x) = \begin{cases}
\displaystyle \frac{f_X(x)}{\mathbf P(X\in S)}, & \text{if } x \in S,\\
0 & \text{otherwise.}

\end{cases}$$


The main thing this equation is doing is rescaling the PDF of $X$ over the reduced set of $S$. The book shows this well:

![[IMG_0694.jpeg]]

---

 When **multiple random variables** are involved, a very similar form is used:
$$f_{X,Y|C}(x,y) = \begin{cases}
\displaystyle \frac{f_{X,Y}(x,y)}{\mathbf P(C)}, &\text{if } (x,y) \in S,\\
0, &\text{otherwise.}
\end{cases}$$

where $C = \{(X,Y) \in S\}$ . 

---

The **Total Probability Theorem** plays a role here. If the subsets $S_1, S_2, \dots, S_n$ form $\Omega$ when coalesced, i.e

$$\bigcup_i^nS_i= \Omega$$
then it is true that:

$$f_X(x) = \sum_{i=1}^n \mathbf P(X \in S_i)f_{X|{X \in S_i}(x)}$$

Or, because $A_i = \{X \in S_i\}$, this can be abbreviated to:

$$f_X(x) = \sum^n_{i=1}\mathbf P(A_i)p_{X|A_i}(x)$$
> This is the form that the book uses

### Conditioning one Random Variable on Another

Unlike the last section, this is again almost identical to how [conditioning worked for discrete variables](2.%20Discrete%20Random%20Variables.md#Conditioning%20one%20variable%20on%20another). 

$$f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$$

In the cases where it's easiest  to find $f_{X|Y}$, we can solve for $f_{X,Y}(y)$ using:

$$f_{X,Y}(x,y) = f_{X|Y}(x,y)f_Y(y) = f_{Y|X}(y,x) f_X(x)$$
^conditioning-expanded
When there are **multiple random variables**, you have some stuff like: ^a3e941

$$f_{X,Y|Z} (x,y|z)= \frac{f_{X,Y,Z}(x,y,z)}{f_Z(z)}, \quad \text{if }f_Z(z) > 0,$$

$$f_{X|Y,Z} (x|y,z)= \frac{f_{X,Y,Z}(x,y,z)}{f_{Y,Z}(y,z)}, \quad \text{if } f_{Y,Z}(y,z) > 0.$$

And a form of the [multiplication rule](1.%20Sample%20Space%20&%20Probability.md#^cond-multiplication) is:

$$f_{X,Y,Z}(x,y,z) = f_{X|Y,Z}(x|y,z)f_{Y|Z}(y|z)f_Z(z)$$

### Conditional Expectations

Derivations are provided on p.174


By definition:
$$\mathbf E[X|A] = \int_{-\infty}^ \infty x f_{X|A}(x)\,dx$$
and
$$\mathbf E[X|Y=y] = \int_{- \infty}^ \infty x f_{X|Y}(x|y)\, dx$$

The analogs for the [Expected Value Rule](2.%20Discrete%20Random%20Variables.md#Expected%20Value%20Rule) are:
$$\mathbf E[g(X)|A] = \int_{- \infty}^ \infty g(x)f_{X|A}(x)\,dx$$
and
$$\mathbf E[g(X)|Y=y] = \int_{- \infty} ^ \infty g(x)f_{X|Y}(x|y)\,dx$$
Then, the **total expectation theorem** is similar to [that of PMFs](2.%20Discrete%20Random%20Variables.md#^total-expectation-theorem):

$$\mathbf E[X] = \sum_i \mathbf P(A_i)\mathbf E[X|A_i]\qquad \forall i$$

$$\mathbf E[X] = \int_{-\infty}^{\infty} \mathbf E[X |Y = y]f_Y(y)dy$$

For multiple random variables there is:

$$\mathbf E[g(X,Y)|Y=y] = \int g(x,y)f_{X|Y}(x|y) \,dx$$

$$\mathbf E[g(X,Y)] = \int \mathbf E[g(X,Y)|Y=y]f_Y(y)\,dy$$

### Independence

This is fully analogous with independence of PMFs.

Two random variables are independent if:
$$f_{X,Y}(x,y) = f_X(x)f_Y(y), \quad \text{for all } x,y. $$
We can see from the above [expanded conditioning rule](#^a3e941) that, if $f_{X|Y}(x|y) = f_X(x)$, we get the same rule:



$$\begin{aligned}
f_{X,Y}(x,y)&= \underbrace{f_{X|Y}(x|y)}_{\text{reduces to} f_X(x)}f_Y(y)\\
&= f_X(x)f_Y(y)
\end{aligned}
$$

The expectation & variance properties remain the same tooâ€”those being:

$$\mathbf E[XY] = \mathbf E[X]\mathbf E[Y]$$
$$\mathbf E[g(X)h(Y)] = \mathbf E[g(X)]\mathbf E[h(Y)]$$
$$\text{var}(X+Y) = \text{var}(X) + \text{var}(Y)$$
^independent-variance

## Covariance

>  Chapter 4.2 in book; Notes 10 in class.

The covariance gives a quantity that is similar to the [Variance](#Expectation,%20Variance) of a random variable, except it takes two variables. Its definition is:

$$\text{cov}(X,Y) = \mathbf E\big[(X-\mathbf E[X])(Y - \mathbf E[Y]) \big] = \mathbf E[XY]-\mathbf E[X] \mathbf E[Y]$$

If $X$ and $Y$ are independent:

$$\text{cov}(X,Y)=0$$

If the covariance is taken of the same variable twice:

$$\text{cov}(X,X) = \text{var}(X)$$

When $\text{cov}({X,Y})>0$, the two variables tend to have the *same sign*; when $\text{cov}({X,Y})<0$, they tend to have *opposite signs*. 

Using covariance, we can expand the [independent variance](#^independent-variance) equation to work with *all* random variables:

$$\text{var}(X+Y)=\text{var}(X)+\text{var}(Y) + 2 \text{cov}(X,Y).$$

### Correlation Coefficient

The covariance will change depending on how the data is normalized. To alleviate this problem altogether, the *correlation coefficient* $\rho$ can be used:

$$\rho(X,Y) = \frac{\text{cov}(X,Y)}{\sqrt{\text{var}(X)\text{var}(Y)}}$$

This coefficient will always be in $[-1, 1]$. The sign indicates whether or not the variables trend in the same or opposite directions, and how close the variable is to $\pm 1$ shows the *strength* of the correlation. For this reason, when $\rho \approx 0$, we say $X$ and $Y$ are **uncorrelated**. 

Intuitively, *independent variables will always be uncorrelated*. However, not all uncorrelated variables are necessarily independent!

To stress the point.
>Not all uncorrelated variables are necessarily independent.







## 3.6 The Continuous Bayes' Rule







