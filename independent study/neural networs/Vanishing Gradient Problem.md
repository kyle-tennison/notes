# Vanishing Gradient Problem

Because in [Back Propagation](Back%20Propagation.md), the partial derivatives of weights in earlier layers are computed after many multiplications (chain rule), the gradients of earlier weights tend to shrink greatly in magnitude, meaning that backpropagation has little to no influence on these earlier weights. 


### References
- https://en.wikipedia.org/wiki/Vanishing_gradient_problem


