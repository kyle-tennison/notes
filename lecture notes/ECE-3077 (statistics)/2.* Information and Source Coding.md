# 2.* Information and Source Coding

## Definition of Information

Claude Shannon came up with **Shannon's Information Theory**. What follows is a preliminary exploration of its application to messages.

We can think of information as the **reduction of uncertainty**, which we can use statistics (namely [Discrete Random Variables](2.%20Discrete%20Random%20Variables.md)) to describe.



![](excalidraw-2025-10-19-21.24.58.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-10-19-21.24.58.excalidraw.md)%%

This is founded on the idea of a message starting at a source and arriving at a sink. 

At the source there is some probability $p_A(a_k)$ of the message $A$ taking on a certain value. This may be anything from a number to a string. For instance, the source may be reporting on the status of a coin flip; here, $p_A(a_1) = p_A(a_2) = \frac 12$ . 

What is important here is that *A is the "message."* There is some amount of **information** associated with that message, and it depends on the probability of $A$ taking on a specific value. Mathematically, we say:

$$\text{Information} = - \log _2 p_A(a_k)\; \text{bits}$$
This shows that the information is in units of bits. For the coin flipping example:

$$\text{Information} = - \log_2 (0.5) = 1 \text{ bit}$$

Which makes sense; there is only one bit of information necessary to report the outcome of the experiment. 


If $p_A(a_k)=1$, the *information will be zero because the outcome is already known*. 

If $p_A(a_1) > p_A(a_2)$, then it will take more information to send $A=a_2$ than $A=a_1$. You can think of us reserving small common outcomes, and big numbers to uncommon outcomes, such that we are most often sending small numbers and saving bandwidth. 




## Entropy

The entropy $H(A)$ of a message $A$ is the **expected amount of information**, i.e.

$$\begin{aligned}\mathbf H(A) &= \mathbf E[- \log _2 p_A(A) ]\\&=- \sum_{k}p_A(a_k)\log_2 p_A(a_k)\;\text{bits}\end{aligned}$$

When the **PMF is uniform** $p_A(a_1) = p_A(a_2) = \cdots = p_A(a_K)$, the entropy is simply:

$$\mathbf H(A) = \log_2 (K)\;\text{bits}$$

This is the **highest entropy case** because all events are equally unlikely.

For example, the entropy of a coin flip is $\log _2 (2) = 1\; \text{bits}$. We can do this because the coin is fair ($\mathbf P(\text{heads})=\mathbf P(\text{tails})$). 

## Source Coding

A **codebook** maps outcomes to bits:

$a_1 \to 0$
$a_2 \to 1$ 
$a_3 \to 01$
etc.

If we define a function $B(a_k)$ that returns the number of bits required to encode a certain message, we would get something like:

$$B(a_1) = 1, \quad B(a_2) =1, \quad B(a_3) = 2$$

The *expectation* of this gives us the average number of bits that it would take to encode a message:

$$\overline B(A) = \mathbf E[B(A)] = \sum_{k=1}^KB(a_k)p_A(a_k)\;\text{bits}$$

Shannon's theorem tells us:
1. For any valid codebook, it is true that:
	$$\overline B(A) \ge \mathbf H(A)$$
2. There exists a valid codebook for which:
	$$\overline B(A) < \mathbf H(A) +1$$


## Huffman Coding

With Huffman coding, you:

1. Sort all the messages by probability in descending order.
2. Assign a `0` to the least likely message and a `1` to the second least likely message. Then, combine the two into a single node with the combined probability:
![](excalidraw-2025-10-20-08.44.26.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-10-20-08.44.26.excalidraw.md)%%
3. Repeat this until there there are no messages remaining
![](excalidraw-2025-10-20-08.49.13.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-10-20-08.49.13.excalidraw.md)%%

Now, we can go right -> left down the tree to get our codebook:

- $a_1 \to 0$
- $a_2 \to 10$
- $a_3 \to 11$. 

Notice how we reserve low bits for the most common $a_1$. What's also neat about this is that **no extra delimiter is necessary;** it's effectively built into the code.

Consider this Huffman code:

![](excalidraw-2025-10-20-08.52.30.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-10-20-08.52.30.excalidraw.md)%%

The codebook is:

- $a_1 \to 0$
- $a_2 \to 10$
- $a_3 \to 110$
- $a_4 \to 1110$
- $a_5 \to 1111$

Then from a code like:

$$011101111010110$$

it's relatively easy to decode:

$$\underbrace{0}_{a_1}\underbrace{1110}_{a_4}\underbrace{1111}_{a_5}\underbrace{0}_{a_1}\underbrace{10}_{a_2}\underbrace{110}_{a_3}$$

$$\text{Total Message} = [a_1, a_4, a_5,a_1, a_2, a_3]$$

There is only one way to decode this message. In some cases, such as below, the Huffman coding does look a little more complicated; this happens when there is not an even number of message outcomes, and their probability distribution is less clean:

![[image-54.png]]

