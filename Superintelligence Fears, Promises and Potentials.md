# Superintelligence: Fears, Promises and Potentials

https://jetpress.org/v25.2/goertzel.pdf

---


- Advocates for AI development regulation 
- What if synthetic intelligence is sentience
	- Would this bring emotions too?
- We don't have to look as AI as exclusively reward maximizing
- Progress isn't usually linear in sciecne. We probably are in for a revolution
	- a priori?
- What if there's a third way of AGI that's not LLMs (obv) but also not synthetic? Is such a thing possible.

>If computing speeds double every two years, what happens when computer-based AIs are doing the research? Computing speed doubles every two years. Computing speed doubles every two years of work. Computing speed doubles every two subjective years of work. Two years after Artificial Intelligences reach human equivalence, their speed doubles. One year later, their speed doubles again. Six months – three months – 1.5 months ... Singularity. Plug in the numbers for current computing speeds, the current doubling time, and an estimate for the raw processing power of the human brain, and the numbers match in: 2021.

A perfect instance of how extrapolation is not conclusive.


- If it's gonna cost $5 billion dollars (my number) to simulate one brain, why not just hire a guy.

> Bostrom does not say this in so many words, but alludes to this perspective in dozens of places. E.g. on p. 48, Bostrom notes that achieving human-level AI via whole-brain emulation shouldn’t require any huge breakthroughs, reviewing details such as reconstruction of 3D neuroanatomy from electron microscope images. On p. 38, he says “human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a nontrivial chance of being developed considerably sooner or much later; that it might perhaps fairly soon thereafter result in superintelligence.”
> pg. 62

- We don't know where motivation would come from. They might just kill themselves.
- AI systems need to grow up, so what if we have a frankenstein-like resentment?

> Final goal: “Make us smile” 
> Perverse instantiation: Paralyze human facial musculatures into constant beaming smiles

An inteligent system would clearly understand that this is sadistic. Why should we expect this as the default behavior.
- The author agrees that an hyper-intelligence AGI would never be so stupid
- This was in the paper clip example

Bostrom was an advocate of very carefully developed AI. This contrasts a lot with what we're seeing today.

> One core idea here seems to be that a few brilliant, right-thinking mathematicians and philosophers locked in a basement are most probably our best hope to save humanity from the unwitting creation of Unfriendly AI by teams of ambitious but not-quite-smart-enough AI developers.

The author mentions how strictly goal-driven AI isn't necessarily the case. Why would we think that an AGI would mindlessly pursue infinite optimization.

Continental philosophy follows questions to more questions. You don't get an exact answer; you just reveal the foundation of an idea.

Author thinks that we should be thinking about how we can foster AI as it matures and finds interests.

Open ended intelligence: An agi that develops, learns, self-organizes.


