# 1. Sample Space & Probability


## 1.2 Probability Models


**Sample space** is $\Omega$ 
- This is every possible outcome

The **probability law** $P(\cdot)$ is the likelihood of different events

An **event** $A$ is a subspace of $\Omega$ 

For example:

$$\Omega = \{1,2,3,4,5,6\} $$

An event might be:
- The result is "1"; $A= \{1\}$ 
- The result is off; $A = \{1,3,5\}$
- The result is __ (anything is valid, so long that it is a subspace)


The *probability law* takes an event as a parameter, which is a subset of $\Omega$ 

$$P(\underbrace{\cdot}_{\text{event; subset of }\Omega} )$$

This law evaluates to a scalar, so for a die you may get something like:

$$P(\{1,3,5\}) = \frac{1}{2}$$

---

### Kolmogorov's probability axioms
1. Non-negativity: $P(A) \ge 0 \forall A$ 
2. Additivity: If $A$ and$B$ are disjoint (mutually exclusive), then $P(A \cup B) = P(A) + P(B)$ 
3. Normalization: $P(\Omega) = 1$ 



From these, you can derive the compliment rule:

$$1=P(\Omega) = P(A\cup A^c) = P(A) + P(A^c)$$
$$P(A^c)=1-P(A)$$

^compliment-rule

You can also say:


| More Properties                                                                    |
| ---------------------------------------------------------------------------------- |
| $$P(A_1\cup \cdots \cup A_n) = P(A_1)+P(A_2\cup\cdots \cup A_n) = \sum_i^nP(A_i)$$ |
| $$\text{If } A \subseteq B\text{, then } P(A) \le P(B)$$                           |
| $$P(A \cup B) = P(A)+P(B) - P(A \cap B)$$                                          |
| $$P(A\cup B) \le P(A) + P(B)$$                                                     |
| $$P(A\cup B \cup C) = P(A)+P(A^c \cap B) + P(A^c\cap B^c \cap C)$$                 |


---

### In-Class Example

> Out of the students in the class, 60% love soda, 70% love pizza, and 40%  love both soda and pizza. What is the probability that a randomly selected student loves neither soda nor pizza?


Let us define the sets:

- $A$ = loves soda
- $B$ = loves pizza
- $A \cap B$ = loves soda and pizza

We know that the union of those who love soda and pizza ($A \cup B$) is the number of everyone who likes either soda OR pizza OR both. Thus, the compliment of $A \cup B$, i.e. $(A \cup B)^c$ is the solution to our problem.

$$
P((A\cup B)^c) = 1 - P(A \cup B) = 1 - (P(A) + P(B) - P(A\cap B))
$$

Then, we are told: 
- $P(A) = 0.6$
- $P(B) = 0.7$
- $P(A\cap B) = 0.4$

Therefore, this is:

$$
P((A\cup B)^c) = 1 - (0.6 +0.7-0.4) = \underbrace{0.1}_{\text{Solution}}$$

---


### Types of models

There are 3 types of probability models:
1. Discrete and finite
2. Discrete and infinite 
3. Continuous 

#### Discrete Finite Models

$$|\Omega | = n$$
The size of the subset is some finite value $n$. The number of possible subsets is $2^n$. 

> Are these proper subsets or just subsets.

$\Omega$ can very easily get huge; for instance, the number of possible phone numbers.


#### Discrete Infinite Models

$$\Omega = \{1, 2, 3, \dots \} = \mathbb{N}$$

##### Example

This might be *"the number of coin flips until a tails is found."* There is, theoretically, no upper bound. 

Here, it happens that:

$$P(\text{\{k tosses until "tails"\}}) = P(k) = \left ( \frac{1}{2}\right)^k$$

Notice, it's important that:

$$\sum_{k=1}^\infty\left(\frac{1}{2} \right)^k = 1$$

Which satisfies the law of normalization. 

If, instead, the probability was *"the probability it takes at least 5 coin flips until a tails is found"*. We're now looking for

$$P(\{5,6,7,\dots\})$$

This would be:

$$P(\{5,6,7,\dots\}) = \sum_{k=5}^\infty \left(\frac 12 \right)^k = 1 - \sum_{k=1}^4\left(\frac 12 \right)^k = 1 - \left(\frac 12 + \frac 14 + \frac 18 + \frac 1{16} \right)= \frac{1}{16}$$
#### Continuous Models

There is a *continuum* of possible outcomes.

A simple example, if your choosing a number at random, the probability of that number being in some interval is the "length" of that interval, normalized to the size of $\Omega$. ^a


---

### Discrete uniform law

This states:

If $\Omega$ is finite with $|\Omega| = n$, then for any $A \subseteq \Omega$ , we can say:

$$P(A) = \frac{|A|}{|\Omega|}= \frac{\text{\# of elements in A}}{n}$$


(Don't overthink it)

---

You can expand this to the continuum to say something like, if $\Omega = \left[0, 1 \right]^2$ , then

$$P(A) = \frac{\text{Area}(A) }{\text{Area}(\Omega)}$$

This is related to [what I was saying earlier ](#^a)about the "length" of a 1D problem.


### In-Class example

> Han and Chewbacca have arranged to meet at the cantina at noon. Unfortunately, Han gets delayed by a bounty hunter and Chewbacca loses his watch, so they both are running late. Suppose that they both arrive with delays of anywhere from zero to two hours (with all possible delay combinations equally likely). Whoever gets there first will have a drink, wait for 20 minutes, and will leave if the other has not yet arrived. What is the probability that Han and Chewbacca meet?

![](excalidraw-2025-08-20-13.36.57.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-08-20-13.36.57.excalidraw.md)%%



## 1.3 Independence


Two events $A$ and $B$ are **independent** if:

$$P(A \cap B ) = P(A) P(B)$$


If $X \subset Y$, then the two are not independent. It really comes down to: if knowing one probability influences the probability of something else, then the two are not independent. 


#### Example:

Given:

$$B_1 = B_2 = \{0, 1\}$$

$$P(B_1=0)=P(B_2=0)= 0.5$$

This is equivalent to a coin flip pretty much. $B_1$ and $B_2$ are independent. Then, look at:

$$Z = B_1 \oplus B_2$$

Would this be independent of $B_1$ and $B_2$? 


| $B_1$ | $B_2$ | $Z$ |
| ----- | ----- | --- |
| 0     | 0     | 0   |
| 0     | 1     | 1   |
| 1     | 0     | 1   |
| 1     | 1     | 0   |

From this table, it's evident that $P(Z=0)=0.5$. Now, notice that if you are told what $B_1$ or (but not both) $B_2$ is, you gain no new information about the probability of $Z$; it will still be a 50% chance. For this reason, Z is actually independent of $B_1$ and $B_2$ 


---

You can have pairwise independence, but not group independence. However, the metric of independence remains unchanged for multiple probabilities:

$$P(A_1 \cap A_2 \cap \dots \cap A_n) = P(A_1)P(A_2)\cdots P(A_n)$$

> So, pairwise independence doesn't guarantee group independence, but does group independence guarantee pair independence?


### Conditional Probability

Conditional probability gives the probability of an event $A$, given the occurrence of an event $B$. 

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

It's important to note:

$$P(A|B) \ne P(B|A)$$

---

For independent events, you can say:

$$P(A \cap B) = P(A) \cdot P(B)$$

But, for conditional probability, you need to change this to:

$$P(A\cap B) =P(B) \cdot  P(A | B) = P(A) \cdot P(B|A)$$

You can derive these from the equation above.

--- 

The **multiplication rule** for *independent events*  is:

$$P\left(\bigcap \limits_{i=1}^n A_i \right) = \prod _{i=1}^n P(A_i)$$

Or, for *conditional events*:

$$P \left ( \bigcap_{i=1}^n A_i \right) = P(A_1)\cdot P(A_2|A_1) \cdots P(A_n| A_1 \cap \cdots \cap A_{n-1})$$


For example, for *three events* you would get:

$$P(A_1 \cap A_2 \cap A_3) = P(A_1) P(A_2|A_3) P(A_3|A_1 \cap A_2)$$

---

The **total probability theorem** states that:
^total-prob-theorem

If $A_1, A_2, \dots, A_n$ are *disjoint events* that fully partition the space $\Omega$, meaning that $\Omega = A_1 \cup A_2 \cup \dots \cup A_n$ , then:

$$P(B) = P(A_1 \cap B) + \cdots + P(A_n\cap B)$$

Then, using the definition of conditional probability, this can be re-expressed as:

$$P(B) = P(B|A_1) P(A_1) + \cdots + P(B| A_n) P(A_n)$$
$$= \sum _{i=1}^n P(B|A_i)P(A_i)$$


Where $B \subset \Omega$ :

![](excalidraw-2025-08-27-12.45.11.excalidraw.svg)
%%[ðŸ–‹ Edit in Excalidraw](excalidraw-2025-08-27-12.45.11.excalidraw.md)%%

### Example

> Robert is the star quarterback for your favorite football team. His knee is bothering him, and so there is only a 40% chance he plays in the next game. If he plays, the probability that your team wins is 0.75. If he does not, it is only 0.35. What is the probability that your team wins the game.


- $A$ = Robert Plays
- $A^c$ = Robert does not play
- $B$ = Wins the game

Thus,

$$P(B) = P(A) P(B|A) + P(A^c) P(B|A^c) = (0.4)(0.75) + (0.6)(0.35) = 51\%$$

---

### Monty Hall Problem

The probability of you winning if you don't switch is:

$$
\begin{aligned}
P(B) = &P(B| Car @ D1) \cdot P(Car @ D1) \\
&+ P(B|Car @ D2) \cdot P(Car @ D2)\\ 
&+ P(B|Car @ D3)\cdot P(Car @ D3) \\
&= 1 \cdot \frac 13 + 0 \cdot \frac 13 + 0 \cdot \frac 13 = \frac 13
\end{aligned}
$$

Now, the probability if you don't switch is:

$$
\begin{aligned}
P(B) = &P(B| Car @ D1) \cdot P(Car @ D1) \\
&+ P(B| Car @ D2) \cdot P(Car @ D2)\\ 
&+ P(B|Car @ D3) \cdot P(Car @ D3) \\
&= 0 \cdot \frac 13 + 1 \cdot \frac 13 +  1 \cdot \frac 13 = \frac 23
\end{aligned}
$$

Therefore it is always advantageous to switch.


---

### Example

> Anders and Blake both have coolers; Anders' is filled with & sodas and 3 beers, while Blake's is filled with 2 sodas and 11 beers. The coolers look identical, so you just choose one at random and start pulling out drinks.
> 
> Suppose you pull out one drink. What is the probability that you pull out a soda?


- $S$ = Pull a soda
- $A$ = Choose Anders' cooler
- $B$ = Choose Blake's cooler

$$P(S) = P(S|A) P(A) + P(S|B)P(B)$$

We know:
- $P(A) = 1/2$
- $P(B) = 1/2$
- $P(S|A) = 8/11$
- $P(S|B) = 2/13$

So, $P(S) = \frac 12 \Big( \frac 8{11} \frac{2}{13}\Big) \approx = \_\_$

Now, *what if there are two sodas*?

- $S_1'$ is a soda on the first pull
- $S_2'$ is a soda on the second pull

We are trying to find:

$$P(S_1' \cap S_2 ' | A  ) = P(S_1'|A) \cdot P(S_2' | S_1' \cap A)$$

>  what the fuck is going on


### Bayes' Rule

$$P(A|B) = \frac{P(B|A)\cdot P(A)}{P(B)}$$

 This comes from the multiplication rule:

$$P(A \cap B) = P(A) P(B|A) =P (B) P(A|B)$$


---

### In-Class Example

- $R$ Robert Plays
- $R^c$ Robert does not play
- $W$ Team wins

Given:

- $P(W|R) = 0.75$
- $P(W|R^c) = 0.35$

Find:

$P(R|W)$ 


$$P(R|W) = \frac{P(W|R)\cdot P(R)}{P(W)}$$

Multiplication rule says:

$$P \left ( \bigcap_{i=1}^n A_i \right) = P(A_1)\cdot P(A_2|A_1) \cdots P(A_n| A_1 \cap \cdots \cap A_{n-1})$$

Or, in this case:


$$P(W) = P(W|R)\cdot P(R) + P(W|R^c)\cdot P(R^c)$$

So this is:

$$P(R|W) = \frac{P(W|R)\cdot P(R)}{P(W|R)\cdot P(R) + P(W|R^c)\cdot P(R^c)}$$


$$P(R|W) = \frac{(0.4)(0.75)}{(0.51)}\approx 0.588$$



---

### Example

> 10% of the population loves to read. However, 100% of librarians love to read.
>
> Librarians make up 0.04% of the population. Suppose I tell you that Rachel loves to read. What is the probability that she is a librarian?


- $A$ is a librarian (0.0004)
- $B$ likes to read (0.1)
- $P(B|A)=1$

What is $P(B|A^c)$:




Find $P(A|B)$

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

$$= \frac{(1)(0.0004)}{0.1} = 0.004$$


## 1.6 Counting / Combinatorics

When *order does not matter*, the number of of $k$ combinations

$$\frac{n!}{(n-k)!}\cdot \frac{1}{k!} = \begin{pmatrix}n\\k\end{pmatrix}$$

The right notation is often used to abbreviate. 


### Permutations

When *order does matter*, the number of $k$ permutations in a set of size $n$ is:

$$\frac{n!}{(n-k)!}$$

### Partitions

Given a set of size $n$, $\begin{pmatrix}n\\k\end{pmatrix}$ can be interpreted as the number of ways that the set can be **partitioned**. 

Then, we are finding how many ways we can partition $n$ into $r$ subsets of size $n_i$ such that:

$$n = \sum _{i}^r n_i$$

#### Example

> How many different 6-letter combinations can you make by rearranging the letters in "BANANA."


---

We are trying to partion 6 characters into a subset of 3, 2, 1. 


### Bernoulli Trials

$$P(\text{success})=p$$

$$P(\text{failure}) = 1-p$$

With $n$ independent events, how can we calculate the probability of *exactly* $k$ successes. 

$$p^k(1-p)^{n-k}$$

There are $(n, k)^T$ possible sequences with k successes in n trials.

---

## **Written Notes**

## Permutations

This tells you how many ways you can order $n$ objects. If you are ordering in to a list of the same size, this is simply:

$$n!$$

For example, if I want to go to Nashville, Charlotte, Tampa, and Birmingham, there are $4! = 24$ ways to order the trip. 

For example, if I want to go to all 50 states, there are $50! =10^{62}$ different orders that I can travel. 

### $k$-permutations

If we want an **ordered subset** of length $k$, the formula for the number of permutations becomes:

$$\frac{n!}{(n-k)!}$$

For example, if there are 20 cars in the pinewood derby, there are $20! / (20-3)! = 6840$ different ways that first, second, and third place can be awarded.   

## Combinations

Given a set of size $n$, if we want an **unordered subset** of length $k$, the formula for the number of *combinations* is:

$$\frac{n!}{(n-k)!k!}$$

The notation for this is the "binomial coefficient":

$$\text{"n choose k"} = \binom{n}{k} = \frac{n!}{(n-k)!k!}$$

On a TI-84, you can evaluate this with $$_n C _k$$.

Go `MATH` --> `PROB` --> `nCr` (3).

For example: There are 12 people on a basketball team, but only 5 people play at a time. How many different 5-person lineups are there:

$$\binom{12}{5} = 792$$

### Partitions

The number of combinations $\binom nk$ can be interpreted as the number of ways a set can be *partitioned* into two sets of size $k$ and $n-k$. 

If we want to find how many ways a set can be *partitioned* into $r$ non-overlapping subsets of sizes $n_1, n_2, \dots , n_r$, we can evaluate:

$$\binom{n}{n_1, n_2, \dots , n_r} = \frac{n!}{n_1! \dots n_r!}$$

For example, think of how many ways we can scramble the $8$ numbers to fit within the sets $n_1, n_2, n_3$:

![[IMG_0337.jpeg]]

There would be:

$$\binom{8}{3,4,1} = \frac{8!}{3!4!1!}=280$$


Another example: How many different words can be obtained by rearranging the letters in BANANA:

We want to find the number of unique combinations for this $6$ letter word, but to avoid overlap, we can treat the repeating characters as subsets:

$$\begin{aligned}

(N);& \quad n_1 = 2\\
(A);& \quad n_2 = 3\\
(B);& \quad n_3 = 1
\end{aligned}$$

Evaluating gives:

$$\binom{6}{2,3,1} = \frac{6!}{2!3!1!}=60$$

## Bernoulli Trials

A **Bernoulli trial** is a random experiment with a binary outcome. This means:

$$p=P(\text{success})$$
$$1-p = P(\text{failure})$$
**Bernoulli trials** refers to a sequence of independent binary trials of this nature. 

To solve a question like this, if you count the number of $k$ success against $n$ trials, you can solve for $p$. 

For any experiment that aims at finding a with $k$ successes in $n$ trials, the probability is:

$$P(\text{exactly } k \text{ successes out of } n) = \binom nk p^k (1-p)^{n-k}$$

and

$$P(\text{at least } k \text{ successes out of } n) = \sum_{i=k}^n \binom{n}{i}p^i(1-p)^{n-i}$$

For example: What is the probability that *exactly* 2 out of the next 3 coin flips are heads?

$$P(\text{two heads}) = \binom 3 2 (0.5)^2 (0.5)^1 = \frac 38$$

For example: 

If I am a 70% free throw shooter... 

a) what is the probability that I make *exactly* 6 out of 10 of my next free throws.

b) what is the probability that I make *at least* 6 out of 10 of my next free throws.

For a)

$$P(6 \text{ free throws}) = \binom {10}6 (0.7)^6 (0.3)^4 \approx 0.2001$$

For b)

$$
\begin{aligned}
P(6 \text{ or more free throws}) = \sum_{i=6}^{10}
\end{aligned} \binom {10} 6 (0.7)^i(0.3)^{10-i} \approx 0.8497
$$

